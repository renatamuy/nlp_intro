---
title: "Getting started with Natural Language Processing"
subtitle: "4th session"
author: "Rohan Alexander"
date: "25 June, 2025"
format:
  revealjs:
    slide-number: true
---

# Conceptual

```{r setup}
#| eval: true
#| echo: false

library(tidyverse)
```

---

## Word embeddings

- Many of the tf-idf entries are going to be zero, and this will increase as our corpus grows.
- Word embeddings are a representation of the relationships between words.
- Words that commonly occur close to each other in a corpus will be located closer together in a n-dimensional graph.

::: {.notes}
:::

---

## Word embeddings (cont.)

- In the ngrams situation we knew that New South Wales could actually be `New_South_Wales` because we know it's a place. Embeddings learn that New South Wales should be kept together because "New" and "South" and "Wales" all occur close to each other commonly.
- In the same way that we created n-grams of different sizes: bi-gram: "South Australia", tri-gram: "New South Wales", we can allow different sized windows - smaller are faster, but larger learn context.

::: {.notes}
:::

---

## Word embeddings (cont.)

```{r}
embeddings <-
  tibble(
    word = c("USyd", "UofT", "Sydney", "Toronto", "apple", "banana"),
    x = c(1, 2, 1, 2, 5, 6),
    y = c(3, 3, 2, 2, 1, 1)
  )

ggplot(embeddings, aes(x, y, label = word)) +
  geom_point(size = 5) +
  geom_text(hjust = -0.2, vjust = -0.3, size = 5) +
  scale_x_continuous(limits = c(0, 7), name = "Dimension 1") +
  scale_y_continuous(limits = c(0, 4), name = "Dimension 2") +
  theme(text = element_text(size = 30)) +
  theme_classic()
```

---

## Pre-trained word embeddings (cont.)

- We need a large corpus and compute to create embeddings, and so it's common to use embeddings created by someone else. 
  - GloVe was trained on Wikipedia and news in 2014.
  - ELMo and BERT, both 2018, were trained on books and Wikipedia, and context matters so words with the same spelling could have different representations.
- Those are publicly available through Hugging Face.

::: {.notes}
:::

---

## Large Language Models

- Next word prediction by looking forward and backward turns out to be powerful at scale.
  - The cat in the ____.
  - The ____ in the hat.
- LLMs start with token embeddings and then each layer adds more context.
- A post-training phase compares different results and rewards outcomes that score better.

::: {.notes}
:::

---

## Benchmarks

- Ultimately, LLMs are statistical models.
- To assess performance, we create benchmarks that are collections of known inputs with desired outputs.
- We need to be able to quickly compare the LLM output with the desired output:
  1. Human evaluation
  2. LLM evaluation
  3. Run it!
- Statistical sampling (from Session 1) becomes important in order to evaluate the appropriateness of the benchmark.

::: {.notes}
- This is one reason why LLMs have been so quickly adopted in situations that require code.
- If you want to allow or encourage development on some aspect of interest to you, then the best starting point would be to create a benchmark.
:::

---

## Benchmarks (cont.)

```{r}
tibble(inputs = c("1+1", "1+2", "...", "0981489181+1981230578324"),
       desired_output = c("2", "3", "...", "1,982,212,067,505"),
       )
```

Would this be a good benchmark?


::: {.notes}

:::

---

## What makes a good benchmark?

- Representative sample.
- Clearly answerable questions.
- Ability to generalize outside the benchmark.
- Can evaluate a range of abilities.
- Domain experts helped create it.

::: {.notes}

:::

---

# Coding

---

## OpenAI API interaction

- Create an account `https://platform.openai.com` and get an API key.
- Add the key to your session.
- `minutes_reasons.py`

---

## Can we use this to answer our question?

```{r}
library(tidyverse)

data <- read_csv("data/minutes_main_reasons.csv")

data |> 
  ggplot(aes(x = date, y = main_reason)) +
  geom_point(size = 5) +
  theme_classic() +
  theme(text = element_text(size = 30),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x = "Date",
       y = "Reason")

```

---

## Next steps

- We were somewhat able to answer our initial question.
- To make more progress we would want:
  - Create a representative benchmark (starting by improving the simulation).
  - Scrape all the minutes, not just the most recent 10.
  - Use the benchmark to optimize model and prompt.
  - Improve the research question!
    - Should whether the interest is changing matter? 
    - Should the party in power matter? etc

---

## Concluding remarks

- Always look at the data.
- Manually code a representative selection.
- Simulate and then update.
- Develop benchmarks.

---

## Recommended reading

- Alammar, Jay, and Maarten Grootendorst, *Hands-On Large Language Models*
- Boykis, Vicky, *What are embeddings*
- Huyen, Chip, *AI Engineering*
- Hvitfeldt, Emil, and Julia Silge, *Supervised Machine Learning for Text Analysis in R*
- Jurafsky, Dan, and James H. Martin, *Speech and Language Processing*
- Lewis, Crystal, *Data Management in Large-Scale Education Research*

---

## Contact

- Today's content: <https://github.com/RohanAlexander/nlp_intro>
- Email: <rohan.alexander@utoronto.ca>
- Website: <https://rohanalexander.com>

